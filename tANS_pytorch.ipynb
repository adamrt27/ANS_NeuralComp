{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spread Function\n",
    "The spread function heuristic is defined as indicated in the original ANS paper (https://arxiv.org/abs/1311.2540).\n",
    "\n",
    "## Definition\n",
    "\n",
    "$N_s := \\left\\{ \\frac{1}{2p_s} + \\frac{i}{p_s} : i \\in \\mathbb{N} \\right\\}$\n",
    "\n",
    "> \"These n sets are uniformly distributed with required densities, but they get out of $\\N$ set - to define ANS we need to shift them there, like in Fig. 9. Specifically, to choose a symbol for the succeeding position, we can take the smallest not used element from $N_1, .., N_n$ sets.\"\n",
    "\n",
    "> - Jarek Duda - Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\n",
    "\n",
    "## Functions\n",
    "### `generate_N_s_i`\n",
    "This function generates the $N_{s_i}$ set for a given probability, `p_s`, and index, `i`\n",
    "\n",
    "### `generate_N_s`\n",
    "This function generates the $N_s$ set for a given probability, `p_s`, and length, `l`.\n",
    "\n",
    "### `generate_N`\n",
    "This function generates the $N$ set for a given probability distribution, `prob`, and length, `l`.\n",
    "\n",
    "### `generate_N_iter`\n",
    "This function generates the $N$ set for a given probability distribution, `prob`, and length, `l`. It then returns an iterator that can be used to generate the $N$ set one element at a time, in ascending order, with each element being of the form, $(N_{s_{cur}}, s)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadFunction:\n",
    "    @staticmethod\n",
    "    def generate_N_s_i(p_s, i):\n",
    "        \"\"\"Generates the value of N_s_i according to the original paper.\n",
    "           N_s_i = 1/(2*p_s) + i/p_s\n",
    "\n",
    "        Args:\n",
    "            p_s (float): the probability of a specific symbol\n",
    "            i (int): the index of the N_s value we want to get\n",
    "\n",
    "        Returns:\n",
    "            float: returns the N_s_i value\n",
    "        \"\"\"\n",
    "        return 1/(2*p_s) + i/p_s\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_N_s(p_s, l):\n",
    "        \"\"\"Generates N_s for a specific symbol with a given length.\n",
    "           N_s = [N_s_0, N_s_1, ..., N_s\n",
    "\n",
    "        Args:\n",
    "            p_s (float): the probability of a specific symbol\n",
    "            l (int): length of the N_s we want to generate\n",
    "\n",
    "        Returns:\n",
    "            list: list of N_s values\n",
    "        \"\"\"\n",
    "        return [SpreadFunction.generate_N_s_i(p_s, i) for i in range(l)]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_N(prob, l):\n",
    "        \"\"\"Generates N for a given set of symbols, given their probabilities and the length of the N_s values.\n",
    "\n",
    "        Args:\n",
    "            prob (list): list of probabilities of the symbols\n",
    "            l (int): length of the N_s values we want to generate\n",
    "\n",
    "        Returns:\n",
    "            list of lists: list of N_s values for each symbol\n",
    "        \"\"\"\n",
    "        return [SpreadFunction.generate_N_s(p, l) for p in prob]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_N_iter(prob, l):\n",
    "        \"\"\"Generates a list of tuples with the N values and their indexes, sorted in ascending order.\n",
    "           This is used to iterate over the N values in the order they should be used, as defined in the original paper.\n",
    "\n",
    "        Args:\n",
    "            prob (list): list of probabilities of the symbols\n",
    "            l (int): length of the N_s values we want to generate\n",
    "\n",
    "        Returns:\n",
    "            list of lists: list of tuples with the N values and their indexes, sorted in ascending order with respect to the N values\n",
    "        \"\"\"\n",
    "        N = SpreadFunction.generate_N(prob, l)\n",
    "        N_iter = [[N[i][j], i] for i in range(len(N)) for j in range(len(N[i]))]\n",
    "        N_iter.sort()\n",
    "        return N_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding\n",
    "The decoding algorithm as outlined in the original paper and other online resources (https://ww2.ii.uj.edu.pl/~smieja/teaching/ti/3a.pdf).\n",
    "\n",
    "<img src=\"decoding.png\" alt=\"Decoding Figure\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "\n",
    "## Classes\n",
    "### DecodeTableColumn\n",
    "One column of the decode table, containing the current state `x`, the symbol `sym`, the number of bits to read from the bitstream `nbBits`, and the new state to go to after reading the bits `newX`.\n",
    "\n",
    "### DecodeTable\n",
    "A class representing the whole decode table. Can be built from the following arguments:\n",
    "* `L`, the sum of frequencies, which is also the length of the table, i.e. 11\n",
    "* `s_list`, the list of symbols, i.e. [A,B,C,D]\n",
    "* `L_s`, the list of frequencies, i.e. [4,2,1,4]\n",
    "\n",
    "It also has the following attributes, which are computed:\n",
    "* `table`, the decoding table, which is a list of `DecodeTableColumn` objects\n",
    "* `symbol_spread`, which is the spread of symbols in the table, calculated using the spread function\n",
    "\n",
    "It has the following methods:\n",
    "* `display_table`: Displays the decoding table as a Pandas DataFrame.\n",
    "* `spread_function`: Computes the symbol spread using the spread function.\n",
    "* `get_decoding_table`: Generates the decoding table for the given symbols and length.\n",
    "* `read_bit`: Reads a specified number of bits from the bitstream from **right to left**.\n",
    "* `decode_step`: Performs a single decoding step.\n",
    "* `decode`: Decodes the entire bitstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd\n",
    "# Building Decoding table\n",
    "\n",
    "class DecodeTableColumn:\n",
    "    def __init__(self, x, sym, nbBits, newX):\n",
    "        \"\"\"Initializes the decoding table column\n",
    "\n",
    "        Args:\n",
    "            x (int): the state of the column\n",
    "            sym (void): the symbol for that column, can be of any type\n",
    "            nbBits (int): number of bits to read from bitstream\n",
    "            newX (int): new state after reading nbBits\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.sym = sym\n",
    "        self.nbBits = nbBits\n",
    "        self.newX = newX\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"State: %d, Symbol: %s, nbBits: %d, newX: %d\" % (self.x, self.sym, self.nbBits, self.newX)\n",
    "    \n",
    "    def __list__(self):\n",
    "        return [self.x, self.sym, self.nbBits, self.newX]\n",
    "        \n",
    "class DecodeTable:\n",
    "    def __init__(self, L, s_list, L_s, fast = False):\n",
    "        \"\"\"Initializes the decoding table\n",
    "\n",
    "        Args:\n",
    "            L (int): the length of the table, equal to the culminative frequency of the symbols\n",
    "            s_list (list): list of symbols in the message, for example (A,B,C)\n",
    "            L_s (list): the frequency of each symbol in the message, for example (2,3,4)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.L = L\n",
    "        self.s_list  = s_list\n",
    "        self.L_s = L_s\n",
    "        \n",
    "        # initialize an empty table, with L empty columns\n",
    "        self.table = [DecodeTableColumn(0,0,0,0) for i in range(L)]\n",
    "        \n",
    "        # spread symbols using the spread function\n",
    "        self.symbol_spread = None\n",
    "        if fast:\n",
    "            self.fast_spread(X = 0, step = int((5/8) * self.L + 3))\n",
    "        else:\n",
    "            self.spread_function()\n",
    "        \n",
    "        # generate the decoding table\n",
    "        self.get_decoding_table()\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Decoding table with length: %d, symbols: %s, frequencies: %s\" % (self.L, self.s_list, self.L_s)\n",
    "    \n",
    "    def display_table(self):\n",
    "        \"\"\"Generates a pandas dataframe to display the decoding table\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: a pandas dataframe with the decoding table\n",
    "        \"\"\"\n",
    "        \n",
    "        # make a list of lists for the table\n",
    "        table = []\n",
    "        for i in range(self.L):\n",
    "            table.append(self.table[i].__list__())\n",
    "            \n",
    "        # convert to pandas dataframe\n",
    "        df = pd.DataFrame(table, columns=[\"State\", \"Symbol\", \"nbBits\", \"newX\"])\n",
    "        df.set_index(\"State\", inplace=True)\n",
    "        return df.T\n",
    "\n",
    "    def spread_function(self):\n",
    "        \"\"\"Uses the heuristic spread function to spread the symbols in the decoding table\n",
    "\n",
    "        Returns:\n",
    "            list: a list of symbols spread according to the spread function\n",
    "        \"\"\"\n",
    "        \n",
    "        symbol = [0 for i in range(self.L)]\n",
    "        spread = SpreadFunction.generate_N_iter([i/self.L for i in self.L_s], self.L)\n",
    "        \n",
    "        for X, e in enumerate(spread):\n",
    "            if X < self.L:\n",
    "                symbol[X] = e[1]\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        # update in place\n",
    "        self.symbol_spread = symbol\n",
    "        return symbol\n",
    "    \n",
    "    def fast_spread(self, X, step):\n",
    "        if self.L == 0:\n",
    "            raise ValueError(\"self.L must be greater than 0\")\n",
    "        \n",
    "        symbol = torch.zeros(self.L, dtype=torch.int32)\n",
    "        print(f\"Initialized symbol tensor with size {symbol.size()}\")\n",
    "        print(f\"X: {X}, step: {step}, self.L: {self.L}\")\n",
    "\n",
    "        for s in range(len(self.s_list)):\n",
    "            for i in range(1, self.L_s[s] + 1):\n",
    "                if X < 0 or X >= self.L:\n",
    "                    raise IndexError(f\"Index {X} is out of bounds for dimension 0 with size {self.L}\")\n",
    "                symbol[X] = s\n",
    "                X = (X + step) % self.L\n",
    "\n",
    "        self.symbol_spread = symbol\n",
    "        return symbol\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_bit(bitstream, nbits):\n",
    "        \"\"\"Read nbits from bitstream (a list of bits), and return the bits read and the remaining bitstream.\n",
    "           Reads from right to left.\n",
    "\n",
    "        Args:\n",
    "            bitstream (list): the binary bitstream to read from (list of 0s and 1s)\n",
    "            nbits (int): number of bits to read\n",
    "\n",
    "        Returns:\n",
    "            (list, list): returns (bits, bitstream) where bits is the list of bits read, and bitstream is the remaining bitstream\n",
    "        \"\"\"\n",
    "        # check if nbits == 0\n",
    "        if nbits == 0:\n",
    "            return [], bitstream\n",
    "        # read nbits from bitstream\n",
    "        bits = bitstream[-nbits:]  # get the last nbits\n",
    "        bitstream = bitstream[:-nbits]  # remove the last nbits from the bitstream\n",
    "        return bits, bitstream\n",
    "\n",
    "    def get_decoding_table(self):\n",
    "        \"\"\"Generates the decoding table, and returns. Also updates table in place.\n",
    "\n",
    "        Returns:\n",
    "            list of lists: returns the decoding table as a list of lists\n",
    "        \"\"\"\n",
    "        \n",
    "        # next holds the next x_tmp for each symbol and is incremented after each use\n",
    "        # is initialized to the frequency of each symbol\n",
    "        next = [e for e in self.L_s] \n",
    "        \n",
    "        # calculate R, the number of bits needed to represent L\n",
    "        R = math.log2(self.L)  # L = 2^R\n",
    "        \n",
    "        # generate the decoding table\n",
    "        for X in range(self.L):\n",
    "            # set the x value\n",
    "            self.table[X].x = X\n",
    "            \n",
    "            # set the symbol, given by the spread function\n",
    "            self.table[X].sym = self.symbol_spread[X]\n",
    "\n",
    "            # calculate x_tmp\n",
    "            x_tmp = next[self.symbol_spread[X]]\n",
    "            \n",
    "            # Ensure x_tmp is positive\n",
    "            if x_tmp <= 0:\n",
    "                raise ValueError(f\"x_tmp must be positive but got {x_tmp} at index {X}\")\n",
    "\n",
    "            # Debug print statement\n",
    "            print(f\"x_tmp: {x_tmp} at index {X}\")\n",
    "            \n",
    "            # increment next at the index of the symbol\n",
    "            next[self.symbol_spread[X]] += 1\n",
    "            \n",
    "            print(f\"R: {R}\")\n",
    "            print(f\"log2(x_tmp): {((x_tmp))}\")\n",
    "            print(R - math.floor(math.log2(x_tmp)))\n",
    "            \n",
    "            # calculate the number of bits needed to represent x_tmp\n",
    "            self.table[X].nbBits = int(R - math.floor(math.log2(x_tmp)))\n",
    "            \n",
    "            # calculate the new x value\n",
    "            self.table[X].newX = (x_tmp << self.table[X].nbBits) - self.L\n",
    "        \n",
    "        return self.table\n",
    "    \n",
    "    def decode_step(self, state, bitstream):\n",
    "        \"\"\"A single step in the decoding process, decodes a single symbol from the bitstream\n",
    "\n",
    "        Args:\n",
    "            state (int): the current state of the decoding process, in the range [0, L)\n",
    "            bitstream (list): A bitstream to decode, should be a list of 0s and 1s\n",
    "\n",
    "        Returns:\n",
    "            (void, int, list): returns (decoded symbol, next state, remaining bitstream)\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the symbol\n",
    "        s_decode = self.table[state].sym\n",
    "        \n",
    "        # read into bits from the bitstream, and update the stream back into bitstream\n",
    "        # bits is the bits read from the bitstream\n",
    "        # bitstream is the remaining bitstream after reading bits\n",
    "        bits, bitstream = DecodeTable.read_bit(bitstream, self.table[state].nbBits)\n",
    "        if len(bits) == 0:\n",
    "            bits = 0\n",
    "        else:\n",
    "            bits = int(\"\".join(str(i) for i in bits), 2) # convert bits to int\n",
    "        \n",
    "        # calculate the next state\n",
    "        next_state = self.table[state].newX + bits\n",
    "        \n",
    "        return s_decode, next_state, bitstream\n",
    "    \n",
    "    def decode(self, state, bitstream, state_orig):\n",
    "        \"\"\"decode the entire bitstream, given the initial state\n",
    "           NOTE: the bitstream is decoded right to left\n",
    "\n",
    "        Args:\n",
    "            state (int): the initial state of the decoding process, in the range [0, L)\n",
    "            bitstream (list): the bitstream to decode\n",
    "\n",
    "        Returns:\n",
    "            list: list of decoded symbols, in the order they were decoded\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize empty list to store decoded symbols\n",
    "        decoded = []\n",
    "        \n",
    "        # iterate over the bitstream, decoding each symbol\n",
    "        # stops when the bitstream is empty and the state is equal to the original state\n",
    "        # note: we do state_orig - self.L because the encoding table is from L to 2L\n",
    "        # and we decoding is from 0 to L\n",
    "        while len(bitstream) > 0 or state != (state_orig - self.L):\n",
    "            # decode a single symbol\n",
    "            s_decode, state, bitstream = self.decode_step(state, bitstream)\n",
    "            # append the decoded symbol to the list\n",
    "            decoded.append(self.s_list[s_decode])\n",
    "            #print(state)\n",
    "            \n",
    "        # return the list of decoded symbols\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "The encoding algorithm as outlined in the original paper and other online resources (https://ww2.ii.uj.edu.pl/~smieja/teaching/ti/3a.pdf).\n",
    "\n",
    "<img src=\"encoding.png\" alt=\"Encoding Figure\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "\n",
    "## Encoder\n",
    "\n",
    "A class representing the whole Encoder. Can be built from the following arguments:\n",
    "* `L`, the sum of frequencies, which is also the length of the table, i.e. 11\n",
    "* `s_list`, the list of symbols, i.e. [A,B,C,D]\n",
    "* `L_s`, the list of frequencies, i.e. [4,2,1,4]\n",
    "* `spread`, the spread of symbols in the table, calculated using the spread function\n",
    "\n",
    "It also has the following attributes, which are computed:\n",
    "* `table`, the encoding table, which determines the new state to go to after reading a symbol\n",
    "* `symbol_spread`, which is the spread of symbols in the table, calculated using the spread function\n",
    "* `k`, the number of bits needed to encode symbol, and is adjusted based on frequency of symbols\n",
    "* `nb`, the number of bits from state x to use for encoding symbol\n",
    "* `start`, used to mark the starting position in the encoding table for each symbol\n",
    "\n",
    "It has the following methods:\n",
    "* `display_table`: Displays the decoding table as a Pandas DataFrame.\n",
    "* `use_bits`: Uses the some LSB from the number and returns the rest of the bits and the removed bits.\n",
    "* `setup`: Sets up the encoding table, k, nb, and start.\n",
    "* `encode_step`: Encodes a single symbol, given a state.\n",
    "* `encode`: Encodes the entire message, starting at state=L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "class Encoder:\n",
    "    def __init__(self, L, s_list, L_s, spread):\n",
    "        \"\"\"Initializes the encoder\n",
    "\n",
    "        Args:\n",
    "            L (int): the sum of frequencies, also the length of the table\n",
    "            s_list (list): list of symbols in the message\n",
    "            L_s (list): list of frequencies of the symbols\n",
    "            spread (list): spread of the symbols\n",
    "        \"\"\"\n",
    "        self.L = L\n",
    "        self.s_list = s_list\n",
    "        self.L_s = L_s\n",
    "\n",
    "        # setup the table        \n",
    "        self.table = [0 for i in range(L)]\n",
    "        \n",
    "        # spread symbols using the spread function\n",
    "        self.symbol_spread = spread\n",
    "        \n",
    "        # setup other variables\n",
    "        self.k = None\n",
    "        self.nb = None\n",
    "        self.start = None\n",
    "        \n",
    "        # setup the encoder\n",
    "        self.setup()\n",
    "        \n",
    "    def display_table(self):\n",
    "        \"\"\"Generates a pandas dataframe to display the encoding table\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: a pandas dataframe with the encoding table\n",
    "        \"\"\"\n",
    "        table = []\n",
    "        for i in range(self.L):\n",
    "            table.append(self.table[i])\n",
    "            \n",
    "        df = pd.DataFrame({\"Next\": table,\"State\": [i for i in range(self.L, self.L*2)]})\n",
    "        df.set_index(\"State\", inplace=True)\n",
    "        return df.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def use_bits(num, n_bits):\n",
    "        \"\"\"Uses the first n_bits LSB from num, and returns the bits and the remaining number after removing the bits.\n",
    "\n",
    "        Args:\n",
    "            num (int): the number to extract bits from\n",
    "            n_bits (int): number of bits to extract\n",
    "\n",
    "        Returns:\n",
    "            (list, int): returns (bits_list, num) where bits_list is the list of bits extracted, and num is the remaining \n",
    "            number after removing the bits\n",
    "        \"\"\"\n",
    "        # get the first n_bits (LSB) from num\n",
    "        if n_bits == 0:\n",
    "            return [], num\n",
    "        bits = num & ((1 << n_bits) - 1)\n",
    "        num >>= n_bits\n",
    "        # convert bits to a list of 0s and 1s\n",
    "        bits_list = [int(bit) for bit in bin(bits)[2:].zfill(n_bits)]\n",
    "        return bits_list, num\n",
    "     \n",
    "    def setup(self):\n",
    "        \"\"\"Sets up the encoding table, calculates k, nb, start, and next values\n",
    "\n",
    "        Returns:\n",
    "            list: returns the encoding table list\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate R = log2(L), the number of bits needed to represent L\n",
    "        R = int(math.log2(self.L))\n",
    "        # calculate r = R + 1\n",
    "        r = R + 1\n",
    "        \n",
    "        print(self.L_s)\n",
    "        \n",
    "        # define k, which is the number of bits needed to represent each symbol\n",
    "        self.k = [R - math.floor(math.log2(self.L_s[i])) for i in range(len(self.L_s))]\n",
    "        \n",
    "        # define nb\n",
    "        self.nb = [(self.k[i] << r) - (self.L_s[i] << self.k[i]) for i in range(len(self.L_s))]\n",
    "        \n",
    "        # define start\n",
    "        self.start = [0 for i in range(len(self.L_s))]\n",
    "        for i in range(len(self.L_s)):\n",
    "            self.start[i] = -self.L_s[i] + sum([self.L_s[j] for j in range(i)])\n",
    "        \n",
    "        # define next\n",
    "        next = self.L_s.clone().detach().requires_grad_(False)\n",
    "        \n",
    "        # generate the encoding table\n",
    "        for i in range(self.L):\n",
    "            s = self.symbol_spread[i]\n",
    "            print(self.start[s].item(), next[s].item())\n",
    "            self.table[self.start[s].item() + next[s].item()] = i + self.L\n",
    "            next[s] += 1\n",
    "            \n",
    "        return self.table\n",
    "    \n",
    "    def encode_step(self, state, s):\n",
    "        \"\"\"A single step in the encoding process, encodes a single symbol\n",
    "\n",
    "        Args:\n",
    "            state (int): the current state of the encoding process, should be in the range [L,2L)\n",
    "            s (void): the symbol to encode, can be of any type, but must be in the s_list\n",
    "\n",
    "        Returns:\n",
    "            (list, int): returns (bitstream, next state) where bitstream is the list of bits encoded, and next state is the next state\n",
    "        \"\"\"\n",
    "        \n",
    "        r = int(math.log2(self.L * 2))\n",
    "        \n",
    "        nbBits = (state + self.nb[s]) >> r\n",
    "                \n",
    "        bitstream, state = Encoder.use_bits(state, nbBits)\n",
    "        \n",
    "        print(state.item(),self.start[s].item())\n",
    "        state = self.table[self.start[s].item() + state.item()]\n",
    "        \n",
    "        return bitstream, state\n",
    "    \n",
    "    def encode(self, data):\n",
    "        \"\"\"Encodes the entire data, and returns the bitstream\n",
    "\n",
    "        Args:\n",
    "            data (list): a list of symbols to encode, all symbols should be in the s_list\n",
    "\n",
    "        Returns:\n",
    "            (list, int): returns (bitstream, state) where bitstream is the list of bits encoded, and \n",
    "            state is the final state in [L,2L)\n",
    "        \"\"\"\n",
    "        bitstream = []\n",
    "        state = torch.tensor(0)\n",
    "        \n",
    "        # initialize the state\n",
    "        bits, state = self.encode_step(state, np.where(self.s_list == data[0])[0][0])\n",
    "        \n",
    "        # save original state\n",
    "        state_orig = state\n",
    "        \n",
    "        for s in data:\n",
    "            s_orig = s\n",
    "            s = np.where(self.s_list == s)[0][0]\n",
    "            bits, state = self.encode_step(state, s)\n",
    "            bitstream.extend(bits)\n",
    "            \n",
    "        # encode the final state\n",
    "        #bitstream.extend(Encoder.use_bits(state - self.L, int(math.log2(self.L)))[0])\n",
    "            \n",
    "        return bitstream, state, state_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder\n",
    "This class combines the encoding and decoding classes to provide a complete ANS coder. It has the following methods:\n",
    "* `encode`: Encodes a message using the ANS algorithm.\n",
    "* `decode`: Decodes a message using the ANS algorithm.\n",
    "* `encode_decode`: Encodes and then decodes a message to check for correctness.\n",
    "* `encode_decode_string`: Encodes and then decodes a string message to check for correctness.\n",
    "\n",
    "The `encode_decode` method also returns the number of bits saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coder:\n",
    "    def __init__(self, s_list, L_s, fast = False):\n",
    "        \"\"\"Initializes the coder\n",
    "\n",
    "        Args:\n",
    "            s_list (list): list of symbols in the message\n",
    "            L_s (list): list of frequencies of the symbols\n",
    "        \"\"\"\n",
    "        self.s_list = s_list\n",
    "        self.L_s = L_s\n",
    "        self.L = L_s.sum()\n",
    "        print(self.L)\n",
    "                        \n",
    "        # initialize the decoding table\n",
    "        self.decoding_table = DecodeTable(self.L, s_list, L_s, fast= fast)\n",
    "        \n",
    "        # spread symbols using the spread function\n",
    "        self.spread = self.decoding_table.symbol_spread\n",
    "        \n",
    "        # initialize the encoder\n",
    "        self.encoder = Encoder(self.L, s_list, L_s, self.spread)\n",
    "        \n",
    "    def encode(self, data):\n",
    "        \"\"\"Encodes the data using the encoder\n",
    "\n",
    "        Args:\n",
    "            data (list): a list of symbols to encode\n",
    "\n",
    "        Returns:\n",
    "            list: returns the final state in [0,L) and the bitstream\n",
    "        \"\"\"\n",
    "        return self.encoder.encode(data)[1] - self.L, self.encoder.encode(data)[0], self.encoder.encode(data)[2]\n",
    "    \n",
    "    def decode(self, state, bitstream, orig_state):\n",
    "        \"\"\"Decodes the bitstream using the decoding table\n",
    "\n",
    "        Args:\n",
    "            state (int): the initial state of the decoding process\n",
    "            bitstream (list): the bitstream to decode\n",
    "\n",
    "        Returns:\n",
    "            list: returns the decoded symbols\n",
    "        \"\"\"\n",
    "        # note we reverse the decoded symbols since the bitstream is decoded in opposite order\n",
    "        res = self.decoding_table.decode(state, bitstream, orig_state)\n",
    "        res.reverse()\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_bits(number):\n",
    "        if number == 0:\n",
    "            return 1  # Special case for zero\n",
    "        num_bits = 0\n",
    "        while number:\n",
    "            number >>= 1  # Right shift by 1 bit\n",
    "            num_bits += 1\n",
    "        return num_bits\n",
    "    \n",
    "    def encode_decode(self, data, verbose = False):\n",
    "        \"\"\"Encodes and decodes the data, and returns the decoded data\n",
    "\n",
    "        Args:\n",
    "            data (list): a list of symbols to encode and decode\n",
    "\n",
    "        Returns:\n",
    "            list: returns the decoded symbols\n",
    "        \"\"\"\n",
    "        \n",
    "        state, bitstream, orig_state = self.encode(data)\n",
    "        res = self.decode(state, bitstream, orig_state)\n",
    "        # if there is an error \n",
    "        if res != list(data):\n",
    "            print(\"Error in encoding and decoding\")\n",
    "            \n",
    "        # compute how many bits saved\n",
    "        orig_bits = len(list(data)) * Coder.calculate_bits(len(self.s_list))\n",
    "        comp_bits = len(bitstream)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Original bits: %d, Compressed bits: %d, Saved: %d\" % (orig_bits, comp_bits, orig_bits - comp_bits))\n",
    "        \n",
    "        return res, orig_bits / comp_bits\n",
    "    \n",
    "    def encode_decode_string(self, data):\n",
    "        \"\"\"Encodes and decodes the string data, and returns the decoded data\n",
    "\n",
    "        Args:\n",
    "            data (str): a string to encode and decode\n",
    "\n",
    "        Returns:\n",
    "            list: returns the decoded symbols\n",
    "        \"\"\"\n",
    "        data_t = list(data)\n",
    "        return \"\".join(self.encode_decode(data_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random string\n",
    "import random\n",
    "\n",
    "def generate_random_array_uint8(alphabet, frequencies):\n",
    "    # Convert the alphabet to uint8 tensor\n",
    "    alphabet = torch.tensor(alphabet, dtype=torch.uint8)\n",
    "    \n",
    "    # Create the population array based on frequencies\n",
    "    population = torch.repeat_interleave(alphabet, torch.tensor(frequencies, dtype=torch.int32))\n",
    "    \n",
    "    # Shuffle the population to ensure randomness\n",
    "    population = population[torch.randperm(population.size(0))]\n",
    "    \n",
    "    # Return the shuffled tensor\n",
    "    return population\n",
    "\n",
    "def next_power_of_2(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "def generate_random_list2(l, n):\n",
    "    if l <= 0:\n",
    "        raise ValueError(\"Length of the list must be positive.\")\n",
    "    \n",
    "    # Generate the initial tensor of random numbers\n",
    "    random_tensor = torch.randint(1, n+1, (l,))\n",
    "    total_sum = random_tensor.sum().item()\n",
    "    \n",
    "    # Find the next power of 2 greater than or equal to total_sum\n",
    "    target_sum = next_power_of_2(total_sum)\n",
    "    \n",
    "    # Calculate the adjustment needed\n",
    "    adjustment = target_sum - total_sum\n",
    "    \n",
    "    # Ensure the last element remains positive\n",
    "    if random_tensor[-1] + adjustment <= 0:\n",
    "        # Adjust another element if the last element would become non-positive\n",
    "        for i in range(l-1):\n",
    "            if random_tensor[i] + adjustment > 0:\n",
    "                random_tensor[i] += adjustment\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"Cannot adjust the tensor to make the sum a power of 2 while keeping all elements positive.\")\n",
    "    else:\n",
    "        random_tensor[-1] += adjustment\n",
    "\n",
    "    return random_tensor\n",
    "\n",
    "def generate_random_list_target(l, n, target_sum):\n",
    "    \"\"\"\n",
    "    Generates a random tensor of positive integers with a specified length and sum,\n",
    "    while keeping the values within a specified range and maintaining approximate frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    l (int): Length of the tensor to be generated. Must be a positive integer.\n",
    "    n (int): Maximum value for the random integers in the tensor. Must be a positive integer.\n",
    "    target_sum (int): Desired sum of the generated tensor. Must be a positive integer.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor of length `l` with integers summing up to `target_sum`.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If `l` is less than or equal to 0, if `n` is less than or equal to 0, or if `target_sum` is not achievable \n",
    "                with the given length and range.\n",
    "    \"\"\"\n",
    "    if l <= 0:\n",
    "        raise ValueError(\"Length of the tensor must be positive.\")\n",
    "    \n",
    "    # Generate the initial tensor of random numbers\n",
    "    random_tensor = torch.randint(1, n+1, (l,))\n",
    "    initial_sum = random_tensor.sum().item()\n",
    "    \n",
    "    # Calculate the scaling factor\n",
    "    scaling_factor = target_sum / initial_sum\n",
    "    \n",
    "    # Scale the numbers and round them\n",
    "    scaled_tensor = torch.round(random_tensor * scaling_factor).int()\n",
    "    \n",
    "    # Calculate the difference caused by rounding\n",
    "    scaled_sum = scaled_tensor.sum().item()\n",
    "    difference = target_sum - scaled_sum\n",
    "    \n",
    "    # Adjust the scaled tensor to match the target sum\n",
    "    for i in range(abs(difference)):\n",
    "        if difference > 0:\n",
    "            # Increment a random element if the sum is less than the target\n",
    "            idx = random.randint(0, l - 1)\n",
    "            scaled_tensor[idx] += 1\n",
    "        elif difference < 0:\n",
    "            # Decrement a random element if the sum is more than the target\n",
    "            idx = random.randint(0, l - 1)\n",
    "            if scaled_tensor[idx] > 1:  # Ensure the element stays positive\n",
    "                scaled_tensor[idx] -= 1\n",
    "\n",
    "    # Ensure all initially non-zero elements remain non-zero\n",
    "    for i in range(l):\n",
    "        if random_tensor[i] > 0 and scaled_tensor[i] == 0:\n",
    "            # Find a non-zero element to borrow from\n",
    "            non_zero_indices = [j for j in range(l) if scaled_tensor[j] > 1]\n",
    "            if non_zero_indices:\n",
    "                transfer_idx = random.choice(non_zero_indices)\n",
    "                scaled_tensor[transfer_idx] -= 1\n",
    "                scaled_tensor[i] += 1\n",
    "\n",
    "    # Recalculate the sum and adjust if necessary\n",
    "    final_sum = scaled_tensor.sum().item()\n",
    "    difference = target_sum - final_sum\n",
    "    \n",
    "    for i in range(abs(difference)):\n",
    "        if difference > 0:\n",
    "            idx = random.randint(0, l - 1)\n",
    "            scaled_tensor[idx] += 1\n",
    "        elif difference < 0:\n",
    "            idx = random.randint(0, l - 1)\n",
    "            if scaled_tensor[idx] > 1:\n",
    "                scaled_tensor[idx] -= 1\n",
    "    \n",
    "    return scaled_tensor\n",
    "    \"\"\"\n",
    "    Generates a random tensor of positive integers with a specified length and sum,\n",
    "    while keeping the values within a specified range and maintaining approximate frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    l (int): Length of the tensor to be generated. Must be a positive integer.\n",
    "    n (int): Maximum value for the random integers in the tensor. Must be a positive integer.\n",
    "    target_sum (int): Desired sum of the generated tensor. Must be a positive integer.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor of length `l` with integers summing up to `target_sum`.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If `l` is less than or equal to 0, if `n` is less than or equal to 0, or if `target_sum` is not achievable \n",
    "                with the given length and range.\n",
    "    \"\"\"\n",
    "    if l <= 0:\n",
    "        raise ValueError(\"Length of the tensor must be positive.\")\n",
    "    \n",
    "    # Generate the initial tensor of random numbers\n",
    "    random_tensor = torch.randint(1, n+1, (l,))\n",
    "    initial_sum = random_tensor.sum().item()\n",
    "    \n",
    "    # Calculate the scaling factor\n",
    "    scaling_factor = target_sum / initial_sum\n",
    "    \n",
    "    # Scale the numbers and round them\n",
    "    scaled_tensor = torch.round(random_tensor * scaling_factor).int()\n",
    "    \n",
    "    # Calculate the difference caused by rounding\n",
    "    scaled_sum = scaled_tensor.sum().item()\n",
    "    difference = target_sum - scaled_sum\n",
    "    \n",
    "    # Adjust the scaled tensor to match the target sum\n",
    "    for i in range(abs(difference)):\n",
    "        if difference > 0:\n",
    "            # Increment a random element if the sum is less than the target\n",
    "            idx = random.randint(0, l - 1)\n",
    "            scaled_tensor[idx] += 1\n",
    "        elif difference < 0:\n",
    "            # Decrement a random element if the sum is more than the target\n",
    "            idx = random.randint(0, l - 1)\n",
    "            if scaled_tensor[idx] > 1:  # Ensure the element stays positive\n",
    "                scaled_tensor[idx] -= 1\n",
    "\n",
    "    # Ensure all initially non-zero elements remain non-zero\n",
    "    for i in range(l):\n",
    "        if random_tensor[i] > 0 and scaled_tensor[i] == 0:\n",
    "            non_zero_indices = [j for j in range(l) if scaled_tensor[j] > 1]\n",
    "            if non_zero_indices:\n",
    "                # Transfer a value from one of the non-zero elements\n",
    "                transfer_idx = random.choice(non_zero_indices)\n",
    "                scaled_tensor[transfer_idx] -= 1\n",
    "                scaled_tensor[i] += 1\n",
    "\n",
    "    return scaled_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(128)\n",
      "tensor([ 5,  9, 47, 43, 24], dtype=torch.uint8)\n",
      "tensor([0, 1, 2, 3, 4], dtype=torch.uint8)\n",
      "{5: 1, 9: 1, 24: 1, 43: 1, 47: 1}\n",
      "tensor(128)\n",
      "Initialized symbol tensor with size torch.Size([128])\n",
      "X: 0, step: 83, self.L: 128\n",
      "x_tmp: 5 at index 0\n",
      "R: 7.0\n",
      "log2(x_tmp): 6\n",
      "5.0\n",
      "x_tmp: 43 at index 1\n",
      "R: 7.0\n",
      "log2(x_tmp): 44\n",
      "2.0\n",
      "x_tmp: 47 at index 2\n",
      "R: 7.0\n",
      "log2(x_tmp): 48\n",
      "2.0\n",
      "x_tmp: 48 at index 3\n",
      "R: 7.0\n",
      "log2(x_tmp): 49\n",
      "2.0\n",
      "x_tmp: 24 at index 4\n",
      "R: 7.0\n",
      "log2(x_tmp): 25\n",
      "3.0\n",
      "x_tmp: 44 at index 5\n",
      "R: 7.0\n",
      "log2(x_tmp): 45\n",
      "2.0\n",
      "x_tmp: 49 at index 6\n",
      "R: 7.0\n",
      "log2(x_tmp): 50\n",
      "2.0\n",
      "x_tmp: 25 at index 7\n",
      "R: 7.0\n",
      "log2(x_tmp): 26\n",
      "3.0\n",
      "x_tmp: 45 at index 8\n",
      "R: 7.0\n",
      "log2(x_tmp): 46\n",
      "2.0\n",
      "x_tmp: 50 at index 9\n",
      "R: 7.0\n",
      "log2(x_tmp): 51\n",
      "2.0\n",
      "x_tmp: 51 at index 10\n",
      "R: 7.0\n",
      "log2(x_tmp): 52\n",
      "2.0\n",
      "x_tmp: 26 at index 11\n",
      "R: 7.0\n",
      "log2(x_tmp): 27\n",
      "3.0\n",
      "x_tmp: 46 at index 12\n",
      "R: 7.0\n",
      "log2(x_tmp): 47\n",
      "2.0\n",
      "x_tmp: 52 at index 13\n",
      "R: 7.0\n",
      "log2(x_tmp): 53\n",
      "2.0\n",
      "x_tmp: 27 at index 14\n",
      "R: 7.0\n",
      "log2(x_tmp): 28\n",
      "3.0\n",
      "x_tmp: 47 at index 15\n",
      "R: 7.0\n",
      "log2(x_tmp): 48\n",
      "2.0\n",
      "x_tmp: 53 at index 16\n",
      "R: 7.0\n",
      "log2(x_tmp): 54\n",
      "2.0\n",
      "x_tmp: 9 at index 17\n",
      "R: 7.0\n",
      "log2(x_tmp): 10\n",
      "4.0\n",
      "x_tmp: 48 at index 18\n",
      "R: 7.0\n",
      "log2(x_tmp): 49\n",
      "2.0\n",
      "x_tmp: 49 at index 19\n",
      "R: 7.0\n",
      "log2(x_tmp): 50\n",
      "2.0\n",
      "x_tmp: 54 at index 20\n",
      "R: 7.0\n",
      "log2(x_tmp): 55\n",
      "2.0\n",
      "x_tmp: 28 at index 21\n",
      "R: 7.0\n",
      "log2(x_tmp): 29\n",
      "3.0\n",
      "x_tmp: 50 at index 22\n",
      "R: 7.0\n",
      "log2(x_tmp): 51\n",
      "2.0\n",
      "x_tmp: 55 at index 23\n",
      "R: 7.0\n",
      "log2(x_tmp): 56\n",
      "2.0\n",
      "x_tmp: 10 at index 24\n",
      "R: 7.0\n",
      "log2(x_tmp): 11\n",
      "4.0\n",
      "x_tmp: 51 at index 25\n",
      "R: 7.0\n",
      "log2(x_tmp): 52\n",
      "2.0\n",
      "x_tmp: 52 at index 26\n",
      "R: 7.0\n",
      "log2(x_tmp): 53\n",
      "2.0\n",
      "x_tmp: 56 at index 27\n",
      "R: 7.0\n",
      "log2(x_tmp): 57\n",
      "2.0\n",
      "x_tmp: 29 at index 28\n",
      "R: 7.0\n",
      "log2(x_tmp): 30\n",
      "3.0\n",
      "x_tmp: 53 at index 29\n",
      "R: 7.0\n",
      "log2(x_tmp): 54\n",
      "2.0\n",
      "x_tmp: 57 at index 30\n",
      "R: 7.0\n",
      "log2(x_tmp): 58\n",
      "2.0\n",
      "x_tmp: 11 at index 31\n",
      "R: 7.0\n",
      "log2(x_tmp): 12\n",
      "4.0\n",
      "x_tmp: 54 at index 32\n",
      "R: 7.0\n",
      "log2(x_tmp): 55\n",
      "2.0\n",
      "x_tmp: 58 at index 33\n",
      "R: 7.0\n",
      "log2(x_tmp): 59\n",
      "2.0\n",
      "x_tmp: 59 at index 34\n",
      "R: 7.0\n",
      "log2(x_tmp): 60\n",
      "2.0\n",
      "x_tmp: 30 at index 35\n",
      "R: 7.0\n",
      "log2(x_tmp): 31\n",
      "3.0\n",
      "x_tmp: 55 at index 36\n",
      "R: 7.0\n",
      "log2(x_tmp): 56\n",
      "2.0\n",
      "x_tmp: 60 at index 37\n",
      "R: 7.0\n",
      "log2(x_tmp): 61\n",
      "2.0\n",
      "x_tmp: 6 at index 38\n",
      "R: 7.0\n",
      "log2(x_tmp): 7\n",
      "5.0\n",
      "x_tmp: 56 at index 39\n",
      "R: 7.0\n",
      "log2(x_tmp): 57\n",
      "2.0\n",
      "x_tmp: 61 at index 40\n",
      "R: 7.0\n",
      "log2(x_tmp): 62\n",
      "2.0\n",
      "x_tmp: 62 at index 41\n",
      "R: 7.0\n",
      "log2(x_tmp): 63\n",
      "2.0\n",
      "x_tmp: 31 at index 42\n",
      "R: 7.0\n",
      "log2(x_tmp): 32\n",
      "2.0\n",
      "x_tmp: 57 at index 43\n",
      "R: 7.0\n",
      "log2(x_tmp): 58\n",
      "2.0\n",
      "x_tmp: 63 at index 44\n",
      "R: 7.0\n",
      "log2(x_tmp): 64\n",
      "1.0\n",
      "x_tmp: 32 at index 45\n",
      "R: 7.0\n",
      "log2(x_tmp): 33\n",
      "2.0\n",
      "x_tmp: 58 at index 46\n",
      "R: 7.0\n",
      "log2(x_tmp): 59\n",
      "2.0\n",
      "x_tmp: 64 at index 47\n",
      "R: 7.0\n",
      "log2(x_tmp): 65\n",
      "1.0\n",
      "x_tmp: 65 at index 48\n",
      "R: 7.0\n",
      "log2(x_tmp): 66\n",
      "1.0\n",
      "x_tmp: 33 at index 49\n",
      "R: 7.0\n",
      "log2(x_tmp): 34\n",
      "2.0\n",
      "x_tmp: 59 at index 50\n",
      "R: 7.0\n",
      "log2(x_tmp): 60\n",
      "2.0\n",
      "x_tmp: 66 at index 51\n",
      "R: 7.0\n",
      "log2(x_tmp): 67\n",
      "1.0\n",
      "x_tmp: 34 at index 52\n",
      "R: 7.0\n",
      "log2(x_tmp): 35\n",
      "2.0\n",
      "x_tmp: 60 at index 53\n",
      "R: 7.0\n",
      "log2(x_tmp): 61\n",
      "2.0\n",
      "x_tmp: 67 at index 54\n",
      "R: 7.0\n",
      "log2(x_tmp): 68\n",
      "1.0\n",
      "x_tmp: 12 at index 55\n",
      "R: 7.0\n",
      "log2(x_tmp): 13\n",
      "4.0\n",
      "x_tmp: 35 at index 56\n",
      "R: 7.0\n",
      "log2(x_tmp): 36\n",
      "2.0\n",
      "x_tmp: 61 at index 57\n",
      "R: 7.0\n",
      "log2(x_tmp): 62\n",
      "2.0\n",
      "x_tmp: 68 at index 58\n",
      "R: 7.0\n",
      "log2(x_tmp): 69\n",
      "1.0\n",
      "x_tmp: 36 at index 59\n",
      "R: 7.0\n",
      "log2(x_tmp): 37\n",
      "2.0\n",
      "x_tmp: 62 at index 60\n",
      "R: 7.0\n",
      "log2(x_tmp): 63\n",
      "2.0\n",
      "x_tmp: 69 at index 61\n",
      "R: 7.0\n",
      "log2(x_tmp): 70\n",
      "1.0\n",
      "x_tmp: 13 at index 62\n",
      "R: 7.0\n",
      "log2(x_tmp): 14\n",
      "4.0\n",
      "x_tmp: 63 at index 63\n",
      "R: 7.0\n",
      "log2(x_tmp): 64\n",
      "1.0\n",
      "x_tmp: 64 at index 64\n",
      "R: 7.0\n",
      "log2(x_tmp): 65\n",
      "1.0\n",
      "x_tmp: 70 at index 65\n",
      "R: 7.0\n",
      "log2(x_tmp): 71\n",
      "1.0\n",
      "x_tmp: 37 at index 66\n",
      "R: 7.0\n",
      "log2(x_tmp): 38\n",
      "2.0\n",
      "x_tmp: 65 at index 67\n",
      "R: 7.0\n",
      "log2(x_tmp): 66\n",
      "1.0\n",
      "x_tmp: 71 at index 68\n",
      "R: 7.0\n",
      "log2(x_tmp): 72\n",
      "1.0\n",
      "x_tmp: 14 at index 69\n",
      "R: 7.0\n",
      "log2(x_tmp): 15\n",
      "4.0\n",
      "x_tmp: 66 at index 70\n",
      "R: 7.0\n",
      "log2(x_tmp): 67\n",
      "1.0\n",
      "x_tmp: 67 at index 71\n",
      "R: 7.0\n",
      "log2(x_tmp): 68\n",
      "1.0\n",
      "x_tmp: 72 at index 72\n",
      "R: 7.0\n",
      "log2(x_tmp): 73\n",
      "1.0\n",
      "x_tmp: 38 at index 73\n",
      "R: 7.0\n",
      "log2(x_tmp): 39\n",
      "2.0\n",
      "x_tmp: 68 at index 74\n",
      "R: 7.0\n",
      "log2(x_tmp): 69\n",
      "1.0\n",
      "x_tmp: 73 at index 75\n",
      "R: 7.0\n",
      "log2(x_tmp): 74\n",
      "1.0\n",
      "x_tmp: 7 at index 76\n",
      "R: 7.0\n",
      "log2(x_tmp): 8\n",
      "4.0\n",
      "x_tmp: 69 at index 77\n",
      "R: 7.0\n",
      "log2(x_tmp): 70\n",
      "1.0\n",
      "x_tmp: 74 at index 78\n",
      "R: 7.0\n",
      "log2(x_tmp): 75\n",
      "1.0\n",
      "x_tmp: 75 at index 79\n",
      "R: 7.0\n",
      "log2(x_tmp): 76\n",
      "1.0\n",
      "x_tmp: 39 at index 80\n",
      "R: 7.0\n",
      "log2(x_tmp): 40\n",
      "2.0\n",
      "x_tmp: 70 at index 81\n",
      "R: 7.0\n",
      "log2(x_tmp): 71\n",
      "1.0\n",
      "x_tmp: 76 at index 82\n",
      "R: 7.0\n",
      "log2(x_tmp): 77\n",
      "1.0\n",
      "x_tmp: 8 at index 83\n",
      "R: 7.0\n",
      "log2(x_tmp): 9\n",
      "4.0\n",
      "x_tmp: 71 at index 84\n",
      "R: 7.0\n",
      "log2(x_tmp): 72\n",
      "1.0\n",
      "x_tmp: 77 at index 85\n",
      "R: 7.0\n",
      "log2(x_tmp): 78\n",
      "1.0\n",
      "x_tmp: 78 at index 86\n",
      "R: 7.0\n",
      "log2(x_tmp): 79\n",
      "1.0\n",
      "x_tmp: 40 at index 87\n",
      "R: 7.0\n",
      "log2(x_tmp): 41\n",
      "2.0\n",
      "x_tmp: 72 at index 88\n",
      "R: 7.0\n",
      "log2(x_tmp): 73\n",
      "1.0\n",
      "x_tmp: 79 at index 89\n",
      "R: 7.0\n",
      "log2(x_tmp): 80\n",
      "1.0\n",
      "x_tmp: 41 at index 90\n",
      "R: 7.0\n",
      "log2(x_tmp): 42\n",
      "2.0\n",
      "x_tmp: 73 at index 91\n",
      "R: 7.0\n",
      "log2(x_tmp): 74\n",
      "1.0\n",
      "x_tmp: 80 at index 92\n",
      "R: 7.0\n",
      "log2(x_tmp): 81\n",
      "1.0\n",
      "x_tmp: 81 at index 93\n",
      "R: 7.0\n",
      "log2(x_tmp): 82\n",
      "1.0\n",
      "x_tmp: 42 at index 94\n",
      "R: 7.0\n",
      "log2(x_tmp): 43\n",
      "2.0\n",
      "x_tmp: 74 at index 95\n",
      "R: 7.0\n",
      "log2(x_tmp): 75\n",
      "1.0\n",
      "x_tmp: 82 at index 96\n",
      "R: 7.0\n",
      "log2(x_tmp): 83\n",
      "1.0\n",
      "x_tmp: 43 at index 97\n",
      "R: 7.0\n",
      "log2(x_tmp): 44\n",
      "2.0\n",
      "x_tmp: 75 at index 98\n",
      "R: 7.0\n",
      "log2(x_tmp): 76\n",
      "1.0\n",
      "x_tmp: 83 at index 99\n",
      "R: 7.0\n",
      "log2(x_tmp): 84\n",
      "1.0\n",
      "x_tmp: 15 at index 100\n",
      "R: 7.0\n",
      "log2(x_tmp): 16\n",
      "3.0\n",
      "x_tmp: 76 at index 101\n",
      "R: 7.0\n",
      "log2(x_tmp): 77\n",
      "1.0\n",
      "x_tmp: 77 at index 102\n",
      "R: 7.0\n",
      "log2(x_tmp): 78\n",
      "1.0\n",
      "x_tmp: 84 at index 103\n",
      "R: 7.0\n",
      "log2(x_tmp): 85\n",
      "1.0\n",
      "x_tmp: 44 at index 104\n",
      "R: 7.0\n",
      "log2(x_tmp): 45\n",
      "2.0\n",
      "x_tmp: 78 at index 105\n",
      "R: 7.0\n",
      "log2(x_tmp): 79\n",
      "1.0\n",
      "x_tmp: 85 at index 106\n",
      "R: 7.0\n",
      "log2(x_tmp): 86\n",
      "1.0\n",
      "x_tmp: 16 at index 107\n",
      "R: 7.0\n",
      "log2(x_tmp): 17\n",
      "3.0\n",
      "x_tmp: 79 at index 108\n",
      "R: 7.0\n",
      "log2(x_tmp): 80\n",
      "1.0\n",
      "x_tmp: 80 at index 109\n",
      "R: 7.0\n",
      "log2(x_tmp): 81\n",
      "1.0\n",
      "x_tmp: 86 at index 110\n",
      "R: 7.0\n",
      "log2(x_tmp): 87\n",
      "1.0\n",
      "x_tmp: 45 at index 111\n",
      "R: 7.0\n",
      "log2(x_tmp): 46\n",
      "2.0\n",
      "x_tmp: 81 at index 112\n",
      "R: 7.0\n",
      "log2(x_tmp): 82\n",
      "1.0\n",
      "x_tmp: 87 at index 113\n",
      "R: 7.0\n",
      "log2(x_tmp): 88\n",
      "1.0\n",
      "x_tmp: 17 at index 114\n",
      "R: 7.0\n",
      "log2(x_tmp): 18\n",
      "3.0\n",
      "x_tmp: 82 at index 115\n",
      "R: 7.0\n",
      "log2(x_tmp): 83\n",
      "1.0\n",
      "x_tmp: 88 at index 116\n",
      "R: 7.0\n",
      "log2(x_tmp): 89\n",
      "1.0\n",
      "x_tmp: 89 at index 117\n",
      "R: 7.0\n",
      "log2(x_tmp): 90\n",
      "1.0\n",
      "x_tmp: 46 at index 118\n",
      "R: 7.0\n",
      "log2(x_tmp): 47\n",
      "2.0\n",
      "x_tmp: 83 at index 119\n",
      "R: 7.0\n",
      "log2(x_tmp): 84\n",
      "1.0\n",
      "x_tmp: 90 at index 120\n",
      "R: 7.0\n",
      "log2(x_tmp): 91\n",
      "1.0\n",
      "x_tmp: 9 at index 121\n",
      "R: 7.0\n",
      "log2(x_tmp): 10\n",
      "4.0\n",
      "x_tmp: 84 at index 122\n",
      "R: 7.0\n",
      "log2(x_tmp): 85\n",
      "1.0\n",
      "x_tmp: 91 at index 123\n",
      "R: 7.0\n",
      "log2(x_tmp): 92\n",
      "1.0\n",
      "x_tmp: 92 at index 124\n",
      "R: 7.0\n",
      "log2(x_tmp): 93\n",
      "1.0\n",
      "x_tmp: 47 at index 125\n",
      "R: 7.0\n",
      "log2(x_tmp): 48\n",
      "2.0\n",
      "x_tmp: 85 at index 126\n",
      "R: 7.0\n",
      "log2(x_tmp): 86\n",
      "1.0\n",
      "x_tmp: 93 at index 127\n",
      "R: 7.0\n",
      "log2(x_tmp): 94\n",
      "1.0\n",
      "tensor([10, 18, 94, 86, 48], dtype=torch.uint8)\n",
      "246 10\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m value_count_dict \u001b[38;5;241m=\u001b[39m {value\u001b[38;5;241m.\u001b[39mitem(): count\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m value, count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, counts)}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(value_count_dict)\n\u001b[0;32m---> 34\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mCoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphabet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m c_fast \u001b[38;5;241m=\u001b[39m Coder(alphabet, freq, fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# encode a message\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[134], line 21\u001b[0m, in \u001b[0;36mCoder.__init__\u001b[0;34m(self, s_list, L_s, fast)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoding_table\u001b[38;5;241m.\u001b[39msymbol_spread\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# initialize the encoder\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspread\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[128], line 28\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self, L, s_list, L_s, spread)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# setup the encoder\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[128], line 97\u001b[0m, in \u001b[0;36mEncoder.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbol_spread[i]\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart[s]\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mnext\u001b[39m[s]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mnext\u001b[39m[s] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# set up the coder, with symbols and their frequencies\n",
    "alphabet = torch.arange(5, dtype=torch.uint8)\n",
    "avg_saved = 0\n",
    "avg_saved_fast = 0\n",
    "avg_time = 0\n",
    "avg_time_fast = 0\n",
    "\n",
    "n_trials = 1\n",
    "\n",
    "for i in range(n_trials):\n",
    "    freq = generate_random_list_target(5, 10, 128)  # Assuming generate_random_list2 returns a list or tensor\n",
    "    freq = freq.to(torch.uint8)  # Convert to tensor if necessary\n",
    "    \n",
    "    print(freq.sum())\n",
    "    print(freq)\n",
    "    print(alphabet)\n",
    "    \n",
    "    # Example tensor\n",
    "\n",
    "    # Get the value counts\n",
    "    value_counts = torch.bincount(freq)\n",
    "\n",
    "    # Extract values and their counts\n",
    "    values = torch.nonzero(value_counts, as_tuple=True)[0]\n",
    "    counts = value_counts[value_counts > 0]\n",
    "\n",
    "    # Combine values and counts into a dictionary\n",
    "    value_count_dict = {value.item(): count.item() for value, count in zip(values, counts)}\n",
    "\n",
    "    print(value_count_dict)\n",
    "\n",
    "    c = Coder(alphabet, freq, fast=True)\n",
    "    c_fast = Coder(alphabet, freq, fast=False)\n",
    "    \n",
    "    # encode a message\n",
    "    msg = generate_random_array_uint8(alphabet, freq)  # Assuming generate_random_array_uint8 returns a list or tensor\n",
    "    msg_tmp = torch.tensor(msg, dtype=torch.uint8)  # Convert to tensor if necessary\n",
    "\n",
    "    time_start = time.time()\n",
    "    out_tmp, sav = c.encode_decode(msg_tmp)\n",
    "    time_end = time.time()\n",
    "    avg_time += time_end - time_start\n",
    "\n",
    "    time_start = time.time()\n",
    "    out_tmp_fast, sav_fast = c_fast.encode_decode(msg_tmp)\n",
    "    time_end = time.time()\n",
    "    avg_time_fast += time_end - time_start\n",
    "\n",
    "    avg_saved += sav\n",
    "    avg_saved_fast += sav_fast\n",
    "\n",
    "    if not torch.equal(msg_tmp, torch.tensor(out_tmp)) or not torch.equal(msg_tmp, torch.tensor(out_tmp_fast)):\n",
    "        print(\"Error\")\n",
    "        print(freq, msg)\n",
    "        break\n",
    "\n",
    "print(\"Normal Spread:\", (avg_saved / n_trials), avg_time / n_trials)\n",
    "print(\"Fast Spread:\", (avg_saved_fast / n_trials), avg_time_fast / n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m math\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "math.log2(0)\n",
    "math.floor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time diff is -0.0006854772567748135\n"
     ]
    }
   ],
   "source": [
    "print(\"Time diff is\", avg_time/n_trials - avg_time_fast/n_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
